1. 
struct handle_storage {
	struct rwlock lock;               // 读写锁
	uint32_t harbor;                  // 集群ID
	uint32_t handle_index;            // 当前句柄索引
	int slot_size;                    // 槽位数组大小
	struct skynet_context ** slot;    //skynet_context数组
	
	int name_cap;
	int name_count;
	struct handle_name *name;
};


2. 这个函数有点复杂
   uint32_t skynet_handle_register(struct skynet_context *ctx);
   skynet_context_message_dispatch(struct skynet_monitor *sm, struct message_queue *q, int weight)


3. 代码中有个很巧妙的设计，就是s->slot_size-1，它的低位二进制永远都是1。不信你看，刚开始slot_size是4，4-1就是3，
   扩了以后是 8，8-1就是7，然后16,32....。这样的话，和任何数字与操作，都不会丢失“有效的”低位

4. 

struct handle_storage {
    struct rwlock lock;
    uint32_t harbor;
    uint32_t handle_index;
    int slot_size;
    struct skynet_context ** slot;
    int name_cap;
    int name_count;
    struct handle_name *name;
};

slot缓存
      slot成员指向一个保存服务上下文指针的缓存。实际上是一个数组，代码实现了数组大小不足的时候的自动扩容（X2）。
      slot_size 保存了slot缓存的当前大小。
      handle_index保存了slot数组的使用大小，slot中的每个服务按照他们install的顺序存放，如果有服务退出，后面的所有服务上下文自动前移。
      harbor 保持次进程的harbor ID。
name缓存
      name 指向一个服务名字–ID 组合的数组缓存。 同样具备自动扩容功能（X2）。按照name排序（利用strcmp作为排序函数）。
      name_cap 记录缓存大小
      name_count 记录当前缓存使用大小

5 云风的skynet，定义为一个游戏服务器框架，用c + lua基于Actor模型实现。代码极其精简，c部分的代码只有三千行左右。
       整个skynet框架要解决的核心问题是：把一个消息（数据包）从一个服务（Actor）发送给另一个服务（Actor），并接收其返回。
       也就是在同一进程内（作者也强调并非只限于同一进程，因为可能会有集群间的通讯）的一个服务通过类似rpc之类的调用同一进程内的另外一个服务，
       并接收处理结果。而skynet就是处理这些服务间发送数据包的规则和正确性。
       skynet的核心层全部是c来实现。
       当系统启动的时候，会得到一个提前分配好的节点id，我们称之为harbor id，这个id是集群用的，一个集群内可以启动很多个skynet节点，
       每个节点都会分配到唯一的id。
       一个节点（即一个进程）内有很多个服务，服务可以狭义地暂且理解为功能模块。
       当初始化一个服务的时候，会生成一个skynet_context来作为服务的实例；一个唯一(即使是在集群里也是唯一)的服务handle，即服务的唯一id，
       用来识别服务；一个消息队列message_queue;还要向框架注册一个callback，当服务收到有发送来的消息时，通过这个方法传入。

 Skynet框架做两个必要的保证：
      一、一个服务的 callback 函数永远不会被并发。
      二、一个服务向另一个服务发送的消息的次序是严格保证的。


   我用多线程模型来实现它。底层有一个线程消息队列，消息由三部分构成：源地址、目的地址、以及数据块。框架启动固定的多条线程，每条工作线程不断从消息队列取到消息。
   根据目的地址获得服务对象。当服务正在工作（被锁住）就把消息放到服务自己的私有队列中。否则调用服务的 callback 函数。当 callback 函数运行完后，检查私有队列，
   并处理完再解锁。线程数应该略大于系统的 CPU 核数，以防止系统饥饿。（只要服务不直接给自己不断发新的消息，就不会有服务被饿死）
   由于我们是在同一个进程内工作的。所以我对消息传递做了一点优化。对于目前的点对点消息，要求发送者调用 malloc 分配出消息携带数据用到的内存；
   由接受方处理完后调用 free 清理（由框架来做）。这样数据传递就不需要有额外的拷贝了。做为核心功能，Skynet 仅解决一个问题： 把一个符合规范的 C 模块，
   从动态库（so 文件）中启动起来，绑定一个永不重复（即使模块退出）的数字 id 做为其 handle 。模块被称为服务（Service），服务间可以自由发送消息。
   每个模块可以向 Skynet 框架注册一个 callback 函数，用来接收发给它的消息。每个服务都是被一个个消息包驱动，当没有包到来的时候，它们就会处于挂起状态，
   对 CPU 资源零消耗。如果需要自主逻辑，则可以利用 Skynet 系统提供的 timeout 消息，定期触发





6  struct skynet_monitor {
	int version;
	int check_version;
	uint32_t source;
	uint32_t destination;
};


7  struct monitor {
	int count;
	struct skynet_monitor ** m;
	pthread_cond_t cond;
	pthread_mutex_t mutex;
	int sleep;
	int quit;
};

struct skynet_monitor {
	int version;
	int check_version;
	uint32_t source;
	uint32_t destination;
};


struct message_queue {
	struct spinlock lock;
	uint32_t handle;
	int cap;
	int head;
	int tail;
	int release;
	int in_global;
	int overload;
	int overload_threshold;
	struct skynet_message *queue;
	struct message_queue *next;
};

struct global_queue {
	struct message_queue *head;
	struct message_queue *tail;
	struct spinlock lock;
};


8. 
 struct timer_event {
	uint32_t handle;
	int session;
};

struct timer_node {
	struct timer_node *next;
	uint32_t expire;
};

struct link_list {
	struct timer_node head;
	struct timer_node *tail;
};

struct timer {
	struct link_list near[TIME_NEAR];    //  TIME_NEAR  = 256
	struct link_list t[4][TIME_LEVEL];   //  TIME_LEVEL = 64
	struct spinlock lock;
	uint32_t time;
	uint32_t starttime;
	uint64_t current;
	uint64_t current_point;
};



struct socket_server {
	volatile uint64_t time;
	int recvctrl_fd;           //pipe读端
	int sendctrl_fd;           //pipe写端
	int checkctrl;
	poll_fd event_fd;          //epoll/kqueue的fd
	int alloc_id;
	int event_n;               //epoll_wait  返回的事件数
	int event_index;           //单前处理的事件序号
	struct socket_object_interface soi;
	struct event ev[MAX_EVENT];          //64  epoll_wait 返回的事件集合
	struct socket slot[MAX_SOCKET];      //65536 每个socket_server 可以包含多个socket,slot
	char buffer[MAX_INFO];               //128
	uint8_t udpbuffer[MAX_UDP_PACKAGE];  //65536
	fd_set rfds;                         // select监测的fd集
};



struct socket {
	uintptr_t opaque;          //所属服务在skynet中对应的handle
	struct wb_list high;       //高优先级写队列
	struct wb_list low;        //低优先级写队列
	int64_t wb_size;           //写缓冲尚未发送的数据大小
	struct socket_stat stat;
	volatile uint32_t sending;
	int fd;
	int id;                   //用于索引socket_server里的slot数组
	uint8_t protocol;         //使用的协议类型 tcp/udp
	uint8_t type;             //socket 的类型或状态
	uint8_t reading;
	int udpconnecting;
	int64_t warn_size;
	union {
		int size;            //读缓冲预估需要的大小
		uint8_t udp_address[UDP_ADDRESS_SIZE];
	} p;
	struct spinlock dw_lock;
	int dw_offset;
	const void * dw_buffer;
	size_t dw_size;
};


struct socket_lock {
	struct spinlock *lock;
	int count;
};


struct write_buffer {
	struct write_buffer * next;
	const void *buffer;
	char *ptr;
	size_t sz;
	bool userobject;            //共享
	uint8_t udp_address[UDP_ADDRESS_SIZE];
};


struct wb_list {
	struct write_buffer * head;    //写缓冲区的头指针
	struct write_buffer * tail;    //写缓冲区的头指针
};



struct skynet_module {
	const char * name;
	void * module;
	skynet_dl_create create;
	skynet_dl_init init;
	skynet_dl_release release;
	skynet_dl_signal signal;
};


12 struct skynet_module  struct modules 模块结构体 skynet_module.c skynet_module.h

13 struct skynet_context    skynet_server.c   skynet_server.h

struct request_resumepause {
	int id;
	uintptr_t opaque;
};

struct socket_message {
	int id;
	uintptr_t opaque;
	int ud;	// for accept, ud is new connection id ; for data, ud is size of data 
	char * data;
};


#define SOCKET_TYPE_INVALID 0
#define SOCKET_TYPE_RESERVE 1
#define SOCKET_TYPE_PLISTEN 2
#define SOCKET_TYPE_LISTEN 3
#define SOCKET_TYPE_CONNECTING 4
#define SOCKET_TYPE_CONNECTED 5
#define SOCKET_TYPE_HALFCLOSE 6
#define SOCKET_TYPE_PACCEPT 7
#define SOCKET_TYPE_BIND 8


#define READING_PAUSE 0
#define READING_RESUME 1
#define READING_CLOSE 2


















